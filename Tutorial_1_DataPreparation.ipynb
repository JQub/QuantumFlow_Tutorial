{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import functools\n",
    "\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "interest_num = [3,6]\n",
    "ori_img_size = 28\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1\n",
    "inference_batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "# Weiwen: modify the target classes starting from 0. Say, [3,6] -> [0,1]\n",
    "def modify_target(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:\n",
    "                target[j] = idx\n",
    "                break\n",
    "    new_target = torch.zeros(target.shape[0],2)\n",
    "    for i in range(target.shape[0]):        \n",
    "        if target[i].item() == 0:            \n",
    "            new_target[i] = torch.tensor([1,0]).clone()     \n",
    "        else:\n",
    "            new_target[i] = torch.tensor([0,1]).clone()\n",
    "               \n",
    "    return target,new_target\n",
    "\n",
    "# Weiwen: select sub-set from MNIST\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    dataset.targets,_ = modify_target(dataset.targets)\n",
    "    return dataset\n",
    "\n",
    "################ Weiwen on 12-30-2020 ################\n",
    "# Function: ToQuantumData from Listing 1\n",
    "# Note: Coverting classical data to quantum data\n",
    "######################################################\n",
    "class ToQuantumData(object):\n",
    "    def __call__(self, tensor):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        data = tensor.to(device)\n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len, vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = np.float64(input_matrix.transpose(0,1))\n",
    "        u, s, v = np.linalg.svd(input_matrix)\n",
    "        output_matrix = torch.tensor(np.dot(u, v))\n",
    "        output_data = output_matrix[:, 0].view(1, img_size,img_size)\n",
    "        return output_data\n",
    "\n",
    "################ Weiwen on 12-30-2020 ################\n",
    "# Function: ToQuantumData from Listing 1\n",
    "# Note: Coverting classical data to quantum matrix\n",
    "######################################################\n",
    "class ToQuantumMatrix(object):\n",
    "    def __call__(self, tensor):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        data = tensor.to(device)\n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len, vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = np.float64(input_matrix.transpose(0,1))\n",
    "        u, s, v = np.linalg.svd(input_matrix)\n",
    "        output_matrix = torch.tensor(np.dot(u, v))\n",
    "        return output_matrix                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Weiwen on 12-30-2020 ################\n",
    "# Using torch to load MNIST data\n",
    "######################################################\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((ori_img_size,ori_img_size)),\n",
    "                                transforms.ToTensor()])\n",
    "# Path to MNIST Dataset\n",
    "train_data = datasets.MNIST(root='./data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Id: 0, Target: tensor([1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOFklEQVR4nO3da6xV9ZnH8d9Pp15iGwIiBJAZO4pGGTJ0QogGNUwaG8c3Uo0jvqhMNDkm1qSN82JMeVHiJWlGysQbTU6jlBmrTb2NpjHjhZjBRm3ECwgyLWCciiBglIAhpoM88+IszCme9d+HfVsbnu8n2dl7r2evtZ5szo+19l57rb8jQgCOfyc03QCA/iDsQBKEHUiCsANJEHYgib/o58ps89U/0GMR4bGmd7Rlt3257d/b3mr7tk6WBaC33O5xdtsnSvqDpMskbZf0uqTrIuLdwjxs2YEe68WWfb6krRHxXkT8SdKvJF3ZwfIA9FAnYZ8h6YNRz7dX0/6M7SHb62yv62BdADrUyRd0Y+0qfGU3PSKGJQ1L7MYDTepky75d0sxRz8+UtKOzdgD0Sidhf13SLNvftH2SpMWSnulOWwC6re3d+Ig4aPsWSc9JOlHSQxGxqWudAeiqtg+9tbUyPrMDPdeTH9UAOHYQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRF8vJQ0cjTlz5hTr69evL9ZLZ3SuXLmyOO/SpUuL9X379hXrg4gtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwdVlMbBWrVpVrF9//fXFeulve+/evcV5zz///GJ9z549xXqTuLoskBxhB5Ig7EAShB1IgrADSRB2IAnCDiTB+exozDXXXFOsz5w5s2frHh4eLtYH+Th6uzoKu+33Je2X9IWkgxExrxtNAei+bmzZ/z4iPu7CcgD0EJ/ZgSQ6DXtIet72G7aHxnqB7SHb62yv63BdADrQ6W78gojYYXuKpBds/09ErB39gogYljQscSIM0KSOtuwRsaO63y3pKUnzu9EUgO5rO+y2T7P9jcOPJX1H0sZuNQaguzrZjZ8q6Snbh5fzSET8V1e6wnFjwoQJtbXly5cX550xY0axfuDAgWL9kUceqa09/PDDxXmPR22HPSLek/S3XewFQA9x6A1IgrADSRB2IAnCDiRB2IEkOMUVHVm0aFGxftVVV9XWWh1aa2Xu3LnF+rZt2zpa/vGGLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMGQzSiaPXt2sb5hw4a2l71ly5ZifWhozCudfWnt2rXFelYM2QwkR9iBJAg7kARhB5Ig7EAShB1IgrADSXA+e3KrVq0q1i+44IJivdXvNFavXl1bu/fee4vzrl+/vljH0WHLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJz9ODB58uTa2ooVK4rzXnvttcV6NSR3rT179hTrjz32WG2N4+j91XLLbvsh27ttbxw1bZLtF2xvqe4n9rZNAJ0az278LyRdfsS02yStiYhZktZUzwEMsJZhj4i1kj45YvKVkg7/DnK1pEXdbQtAt7X7mX1qROyUpIjYaXtK3QttD0kqX0wMQM/1/Au6iBiWNCxxwUmgSe0eettle5okVfe7u9cSgF5oN+zPSFpSPV4i6enutAOgV1peN972o5IWSposaZekH0v6T0m/lvSXkv4o6ZqIOPJLvLGWxW58G6ZMqf1KRFL5WPaCBQs6WveuXbuK9fnz5xfrH374YUfrx9Gru258y8/sEXFdTenbHXUEoK/4uSyQBGEHkiDsQBKEHUiCsANJcIrrAJg0aVKxftdddxXrnRxe27hxY7F+9dVXF+scWjt2sGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4zj4Abr/99mL9hhtuaHvZBw4cKNZffPHFYn3btm1tr7tTrX5/cNlllxXrr7zySm3tgw8+aKunYxlbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IguPsfTBnzpxiffr06cV6q2GVP/vss9paq0s9P/jgg8V6Ly1btqxYX7x4cbE+a9asYv3VV1+trd10003FeTdt2lSsH4vYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhxn74Ply5cX6xdffHGxfvrppxfr5557bm1tyZIlxXk7ve77vHnzivXHH3+8ttZqKOqTTjqprZ4Ou/DCC2tr5513XnHelMfZbT9ke7ftjaOmLbP9oe23q9sVvW0TQKfGsxv/C0mXjzH93yJibnV7trttAei2lmGPiLWSPulDLwB6qJMv6G6xvaHazZ9Y9yLbQ7bX2V7XwboAdKjdsP9M0tmS5kraKemndS+MiOGImBcR5W9yAPRUW2GPiF0R8UVEHJL0c0nlU6sANK6tsNueNurpdyWVx/0F0LiWx9ltPyppoaTJtrdL+rGkhbbnSgpJ70sqnxx8nJs9e3ax/vnnnxfrp5xySrHeavz1Z5+tPxjy/PPPF+edMWNGsX7jjTcW6zfffHOxPnny5GK9Ka2uSX88ahn2iLhujMnNXfEAQFv4uSyQBGEHkiDsQBKEHUiCsANJOCL6tzK7fyvrslNPPbW21uqSxlOnTi3WJ06s/bWxJOnss88u1l966aXa2muvvVac99JLL2172ZLUz7+fI+3du7dYv/POO2tr999/f3HegwcPttPSQIgIjzWdLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMGlpCvnnHNOsX7JJZfU1latWtXRuqdNm1asf/rpp8X6/v37a2t33313cd5bb721WD/hhPL24NChQ8V6yYEDB4r1uXPnFuvbtm1re90ZsWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4zl7ZunVr2/WLLrqoOO+ZZ55ZrK9YsaJYb3XO+PTp04v1Tpbd6pzx5557ru11v/vuu8U6x9G7iy07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcfZxKp3X3epY9X333Vesn3HGGcV6L6/N3mo46aVLlxbrK1eu7GY76KGWW3bbM22/ZHuz7U22f1BNn2T7BdtbqvvySAcAGjWe3fiDkv45Is6XdKGk79u+QNJtktZExCxJa6rnAAZUy7BHxM6IeLN6vF/SZkkzJF0paXX1stWSFvWoRwBdcFSf2W2fJelbkn4naWpE7JRG/kOwPaVmniFJQx32CaBD4w677a9LekLSDyNinz3m2HFfERHDkoarZRyzAzsCx7pxHXqz/TWNBP2XEfFkNXmX7WlVfZqk3b1pEUA3tNyye2QT/qCkzREx+lzMZyQtkfST6v7pnnTYJ1OmjPkp5Ev33HNPbW3hwoXFeU8++eR2WvrSnj17ivWPPvqotvb00+V/llb1t956q1jHsWM8u/ELJH1P0ju2366m/UgjIf+17Rsl/VHSNT3pEEBXtAx7RPxWUt0H9G93tx0AvcLPZYEkCDuQBGEHkiDsQBKEHUiCU1wrrU71vOOOO2prEyZMKM7b6lLPL7/8crG+b9++Yv2BBx6ore3YsaM4L/Jgyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSbiXlyn+ysq4Ug3QcxEx5lmqbNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiZZhtz3T9ku2N9veZPsH1fRltj+0/XZ1u6L37QJoV8uLV9ieJmlaRLxp+xuS3pC0SNI/SvosIpaPe2VcvALoubqLV4xnfPadknZWj/fb3ixpRnfbA9BrR/WZ3fZZkr4l6XfVpFtsb7D9kO2JNfMM2V5ne11nrQLoxLivQWf765L+W9JdEfGk7amSPpYUku7QyK7+DS2WwW480GN1u/HjCrvtr0n6jaTnImLFGPWzJP0mIv6mxXIIO9BjbV9w0rYlPShp8+igV1/cHfZdSRs7bRJA74zn2/iLJb0s6R1Jh6rJP5J0naS5GtmNf1/STdWXeaVlsWUHeqyj3fhuIexA73HdeCA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBItLzjZZR9L+t9RzydX0wbRoPY2qH1J9Naubvb2V3WFvp7P/pWV2+siYl5jDRQMam+D2pdEb+3qV2/sxgNJEHYgiabDPtzw+ksGtbdB7Uuit3b1pbdGP7MD6J+mt+wA+oSwA0k0Enbbl9v+ve2ttm9rooc6tt+3/U41DHWj49NVY+jttr1x1LRJtl+wvaW6H3OMvYZ6G4hhvAvDjDf63jU9/HnfP7PbPlHSHyRdJmm7pNclXRcR7/a1kRq235c0LyIa/wGG7UslfSbp3w8PrWX7XyV9EhE/qf6jnBgR/zIgvS3TUQ7j3aPe6oYZ/yc1+N51c/jzdjSxZZ8vaWtEvBcRf5L0K0lXNtDHwIuItZI+OWLylZJWV49Xa+SPpe9qehsIEbEzIt6sHu+XdHiY8Ubfu0JffdFE2GdI+mDU8+0arPHeQ9Lztt+wPdR0M2OYeniYrep+SsP9HKnlMN79dMQw4wPz3rUz/Hmnmgj7WEPTDNLxvwUR8XeS/kHS96vdVYzPzySdrZExAHdK+mmTzVTDjD8h6YcRsa/JXkYbo6++vG9NhH27pJmjnp8paUcDfYwpInZU97slPaWRjx2DZNfhEXSr+90N9/OliNgVEV9ExCFJP1eD7101zPgTkn4ZEU9Wkxt/78bqq1/vWxNhf13SLNvftH2SpMWSnmmgj6+wfVr1xYlsnybpOxq8oaifkbSkerxE0tMN9vJnBmUY77phxtXwe9f48OcR0febpCs08o38NklLm+ihpq+/lrS+um1qujdJj2pkt+7/NLJHdKOk0yWtkbSlup80QL39h0aG9t6gkWBNa6i3izXy0XCDpLer2xVNv3eFvvryvvFzWSAJfkEHJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n8P/TUXmy7nYFHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Data: tensor([0.0000, 0.0078, 0.2157, 0.0941, 0.0078, 0.2314, 0.3098, 0.0431, 0.0588,\n",
      "        0.4510, 0.4314, 0.0667, 0.0431, 0.1843, 0.0471, 0.0000])\n",
      "Quantum Data: tensor([0.0000, 0.0098, 0.2691, 0.1174, 0.0098, 0.2887, 0.3866, 0.0538, 0.0734,\n",
      "        0.5627, 0.5383, 0.0832, 0.0538, 0.2300, 0.0587, 0.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Process data by hand, we can also integrate ToQuantumData into transform\n",
    "def data_pre_pro(img):\n",
    "    # Print original figure\n",
    "    img = img\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "    plt.show()\n",
    "    # Print resized figure\n",
    "    image = np.asarray(npimg[0] * 255, np.uint8)    \n",
    "    im = Image.fromarray(image,mode=\"L\")\n",
    "    im = im.resize((4,4),Image.BILINEAR)    \n",
    "    plt.imshow(im,cmap='gray',)\n",
    "    plt.show()\n",
    "    # Converting classical data to quantum data\n",
    "    trans_to_tensor = transforms.ToTensor()\n",
    "    trans_to_vector = ToQuantumData()\n",
    "    trans_to_matrix = ToQuantumMatrix()    \n",
    "    print(\"Classical Data: {}\".format(trans_to_tensor(im).flatten()))\n",
    "    print(\"Quantum Data: {}\".format(trans_to_vector(trans_to_tensor(im)).flatten()))\n",
    "    return trans_to_matrix(trans_to_tensor(im)),trans_to_vector(trans_to_tensor(im))\n",
    "    \n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    torch.set_printoptions(threshold=sys.maxsize)\n",
    "    print(\"Batch Id: {}, Target: {}\".format(batch_idx,target))\n",
    "    quantum_matrix,qantum_data = data_pre_pro(torchvision.utils.make_grid(data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ┌────────┐\n",
      "in_qbit_0: ┤0       ├\n",
      "           │        │\n",
      "in_qbit_1: ┤1       ├\n",
      "           │  Input │\n",
      "in_qbit_2: ┤2       ├\n",
      "           │        │\n",
      "in_qbit_3: ┤3       ├\n",
      "           └────────┘\n",
      "Data to be encoded: \n",
      " tensor([[[0.0000, 0.0098, 0.2691, 0.1174],\n",
      "         [0.0098, 0.2887, 0.3866, 0.0538],\n",
      "         [0.0734, 0.5627, 0.5383, 0.0832],\n",
      "         [0.0538, 0.2300, 0.0587, 0.0000]]], dtype=torch.float64)\n",
      "\n",
      "Data read from the circuit: \n",
      " [0.        +0.j 0.00978642+0.j 0.26912649+0.j 0.11743701+0.j\n",
      " 0.00978642+0.j 0.28869932+0.j 0.38656351+0.j 0.0538253 +0.j\n",
      " 0.07339813+0.j 0.56271902+0.j 0.53825298+0.j 0.08318455+0.j\n",
      " 0.0538253 +0.j 0.22998082+0.j 0.05871851+0.j 0.        +0.j]\n"
     ]
    }
   ],
   "source": [
    "################ Weiwen on 12-30-2020 ################\n",
    "# Do quantum state preparation and compare it with\n",
    "# the original data\n",
    "######################################################\n",
    "\n",
    "# Quantum-State Preparation in IBM Qiskit\n",
    "from qiskit import QuantumRegister, QuantumCircuit, ClassicalRegister\n",
    "from qiskit.extensions import XGate, UnitaryGate\n",
    "from qiskit import Aer, execute\n",
    "import qiskit\n",
    "# Input: a 4*4 matrix (data) holding 16 input data\n",
    "inp = QuantumRegister(4,\"in_qbit\")\n",
    "circ = QuantumCircuit(inp)\n",
    "data_matrix = quantum_matrix\n",
    "circ.append(UnitaryGate(data_matrix, label=\"Input\"), inp[0:4])\n",
    "print(circ)\n",
    "# Using StatevectorSimulator from the Aer provider\n",
    "simulator = Aer.get_backend('statevector_simulator')\n",
    "result = execute(circ, simulator).result()\n",
    "statevector = result.get_statevector(circ)\n",
    "\n",
    "print(\"Data to be encoded: \\n {}\\n\".format(qantum_data))\n",
    "print(\"Data read from the circuit: \\n {}\".format(statevector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
